---
title: "SY19 projet 2"
output: pdf_document
knitr: warning = FALSE, message = FALSE
---

\small

## Dataset Phonemes

Tout d'abord, je traiterai d'abord la base de données. J'ai effectué une analyse en composantes principales sur les données et sélectionné les 100 principaux facteurs pour former une nouvelle base de données. Ensuite, j'ai normalisé les deux ensembles de données et les ai divisés en ensembles d'entra??nement et de test selon un rapport de 2:1.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
data_phonemes <- read.csv("phoneme_train.txt",header = TRUE,sep=" ",row.names = NULL)
data_phonemes <- data_phonemes[2:length(data_phonemes)]
# générer 2 base de donnée pour train
x<-data_phonemes
pca <- prcomp(x[,-257])
X<-pca$x[,1:100]
data_pca <- data.frame(X)
data_pca['y']<-data_phonemes['y']
#scale
data_phonemes[,-257]<-apply(data_phonemes[,-257], 2, scale)
data_pca[,-101]<-apply(data_pca[,-101], 2, scale)
n <- nrow(data_phonemes)
train<-sample(1:n,round(2*n/3))
data_train <- data_phonemes[train,]
data_test <- data_phonemes[-train,]
data_pca_train <- data_pca[train,]
data_pca_test <- data_pca[-train,]
ntest<-nrow(data_test)
```

Ensuite, nous avons utilisé de nombreux modèles pour apprendre et tester sur les données. Pour les données originales, nous avons utilisées : KNN, Naive Bayes, LDA, Random Forests, MDA. Pour les données de l'analyse en composantes principales, nous avons utilisé les modèles suivants: KNN, LDA, QDA, Random Forests.

```{r, warning=FALSE, message=FALSE, fig.width = 5, fig.height=3, fig.align = "center", fig.show='hold', cache=TRUE}

#knn
library(FNN)
ERR.knn<-matrix(0,20,2)
for(k in 1:20){
  knn.class<-knn(data_train[,-257],
                 data_test[,-257],data_train$y,k=k)
  knn.class_1<-knn(data_pca_train[,-101],
                   data_pca_test[,-101],data_pca_train$y,k=k)
  ERR.knn[k,1]<-mean(data_test$y != knn.class)
  ERR.knn[k,2]<-mean(data_pca_test$y != knn.class_1)
}

err.knn.min=min(ERR.knn)
print(err.knn.min)

k_min=which(ERR.knn[,1]==min(ERR.knn[,1]),arr.ind=TRUE)

# Naive Bayes
library(naivebayes)
fit.nb<- naive_bayes(as.factor(y)~.,data=data_train)
pred.nb<-predict(fit.nb,newdata=data_test,type="class")
perf <-table(data_test$y,pred.nb)
print(perf)
err.nb <-1-sum(diag(perf))/ntest
print(err.nb)

# LDA
library(MASS)
fit.lda<- lda(y~.,data=data_train)
pred.lda<-predict(fit.lda,newdata=data_test)
perf <-table(data_test$y,pred.lda$class)
print(perf)
err.lda <- 1-sum(diag(perf))/ntest
print(err.lda)

# LDA_pca
fit.lda_pca<- lda(y~.,data=data_pca_train)
pred.lda_pca<-predict(fit.lda_pca,newdata=data_pca_test)
perf_pca <-table(data_pca_test$y,pred.lda_pca$class)
print(perf_pca)
err.lda_pca <- 1-sum(diag(perf_pca))/ntest
print(err.lda_pca)

# QDA
fit.qda<- qda(y~.,data=data_pca_train)
pred.qda<-predict(fit.qda,newdata=data_pca_test)
perf <-table(data_pca_test$y,pred.qda$class)
print(perf)
err.qda <-1-sum(diag(perf))/ntest
print(err.qda)
#performance très mal


# Random forests
library(randomForest)
fit.RF<-randomForest(as.factor(y) ~ .,data=data_train,mtry=16)
pred.RF<-predict(fit.RF,newdata=data_test,type="class")
err.RF<-1-mean(data_test$y==pred.RF)
print(err.RF)

#Random forests en pca (performance mieux)
fit.RF.pca<-randomForest(as.factor(y) ~ .,data=data_pca_train,mtry=10)
pred.RF.pca<-predict(fit.RF.pca,newdata=data_pca_test,type="class")
err.RF.pca<-1-mean(data_pca_test$y==pred.RF.pca)
print(err.RF.pca)

#mda
library(mda)
fit.mda<- mda(y~.,data=data_train)
pred.mda<-predict(fit.mda,newdata=data_test)
perf <-table(data_test$y,pred.mda)
print(perf)
pred.mda <-1-sum(diag(perf))/ntest
print(pred.mda)
```

Parmi eux, nous pouvons voir que KNN, LDA, LDA en PCA, MDA, Random forests ont une plus grande précision.

Ensuite, nous apprenons et testons sur le modèle 'Neural Network'. Nous devons déterminer le meilleur choix pour le paramètre "decay", nous utilisons donc une 5-fold validation croisée. Dans ce processus, nous avons sélectionné une valeur "size" plus petite pour accélérer la vitesse d'apprentissage.

```{r, warning=FALSE, message=FALSE, fig.width = 5, fig.height=3, fig.align = "center", cache=TRUE}
library(nnet)
library("caret")
K<-5
ntrain<-nrow(data_phonemes)
lambda<-c(0,0.01,0.1,1,5,10,100)
N<-length(lambda)
err<-matrix(0,N)
folds<-createFolds(y=data_phonemes[,257],k=K)
for(i in (1:N)){
  for(k in (1:K)){
    nn<- nnet(as.factor(y) ~ ., data=data_phonemes[-folds[[k]],],size=5, 
              decay=lambda[i],trace=FALSE,MaxNWts = 100000,maxit = 1000)
    pred<-predict(nn,newdata=data_phonemes[folds[[k]],],type="class")
    err[i]<-err[i]+ (1-mean(data_phonemes[folds[[k]],]$y==pred))
  }
  err[i]<-err[i]/K
}
plot(lambda,err,type='l')
lambda.opt<-lambda[which.min(err)] # Best decay coefficient
lambda.opt
```

Par la suite, nous avons effectué une 5-fold validation croisée avec les modèles précédemment sélectionnés avec de faibles taux d'erreur et 'Neural Network' pour sélectionner le modèle final à utiliser.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
K<-5
ntrain<-nrow(data_phonemes)
err<-matrix(0,6,K)
folds<-createFolds(y=data_phonemes[,257],k=K)
for(k in (1:K)){
  #knn
  knn.class<-knn(data_phonemes[-folds[[k]],-257],
                 data_phonemes[folds[[k]],-257],data_phonemes[-folds[[k]],257],k=k_min)
  err[1,k]<-mean(data_phonemes[folds[[k]],257] != knn.class)
  
  # LDA
  fit.lda<- lda(y~.,data=data_phonemes[-folds[[k]],])
  pred.lda<-predict(fit.lda,newdata=data_phonemes[folds[[k]],])
  err[2,k]<-mean(data_phonemes[folds[[k]],257] != pred.lda$class)
  
  # LDA_pca
  fit.lda.pca<- lda(y~.,data=data_pca[-folds[[k]],])
  pred.lda.pca<-predict(fit.lda.pca,newdata=data_pca[folds[[k]],])
  err[3,k]<-mean(data_pca[folds[[k]],101] != pred.lda.pca$class)
  
  #mda
  fit.mda<- mda(y~.,data=data_phonemes[-folds[[k]],])
  pred.mda<-predict(fit.mda,newdata=data_phonemes[folds[[k]],])
  err[4,k]<-mean(data_phonemes[folds[[k]],257] != pred.mda)
  
  #random-foret
  fit.RF<-randomForest(as.factor(y) ~ .,data=data_phonemes[-folds[[k]],],importance=TRUE,mtry=16)
  pred.RF<-predict(fit.RF,newdata=data_phonemes[folds[[k]],],type="class")
  err[5,k]<-mean(data_phonemes[folds[[k]],257] != pred.RF)
  
  #nn
  nn<- nnet(as.factor(y) ~ ., data=data_phonemes[-folds[[k]],],size=10, 
            decay=lambda.opt,trace=FALSE,MaxNWts = 100000,maxit = 300)
  pred<-predict(nn,newdata=data_phonemes[folds[[k]],],type="class")
  err[6,k]<-mean(data_phonemes[folds[[k]],257] != pred)
}
ERR <- data.frame(t(err))
names(ERR) <- c("KNN","LDA","LDA_PCA","MDA","RF","NN")
```

```{r, fig.width = 5, fig.height=3, fig.align = "center", cache=TRUE}
boxplot(ERR)
```

On peut voir que LDA en PCA, Random Forests et Neural Network ont une plus grande précision. étant donné que Neural Network peut continuer à augmenter le nombre d'unités dans la couche cachée pour améliorer la précision du modèle, nous avons choisi d'utiliser Neural Network comme modèle final et augmenté la valeur du paramètre 'size'.

## Dataset Robotics

Faire avec ksvm

On va analyser le dataset Robotics. Il y a 4000 observations avec 8 prédicteurs et 1 variable de réponse. On peut essayer de réduire le nombre de prédicteurs afin d'obtenir un modèle plus simple.

```{r, warning=FALSE, message=FALSE, results='hide', cache=TRUE}
library(ggplot2)
library(e1071)
library(splines)
library(kernlab)
library(nnet)
library(dplyr)
library(caret)
set.seed(123)
data <- read.table("robotics_train.txt", header = TRUE, sep = " ")
X <- subset(data, select = -y)
y <- data$y
fit <- lm(y ~ .,data)
summary(fit)
```

On remarque que tous les prédicteurs semblent signifiants pour prédire y. On a pu le confirmer avec regsubsets le R2 et le BIC n'est pas assez important pour supprimer des prédicteurs en backward et forward.

Après cette première analyse du dataset, on peut s'intéresser aux méthodes que l'on va utiliser pour prédire de nouvelles valeurs. Afin de faire une première analyse rapidement, on va sélectionner un base d'apprentissage et une base de tests par l'approche hold-out sur plusieurs base d'apprentissage différentes et de tailles différentes.

```{r, cache=TRUE}
# Diviser les données en un ensemble d'entraînement et un ensemble de test
train_index <- sample(1:nrow(data), 0.8 * nrow(data))
data.train <- data[train_index, ]
data.test <- data[-train_index, ]
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]
```

Les méthodes choisis pour ce dataset sont : la régression linéaire, la régression généralisée, les SVM, les B-splines cubiques, les KSVM et un modèle de réseaux de neurones. On va maintenant faire une cross-validation sur ces méthodes.

```{r, warning=FALSE, message=FALSE, fig.align = "center", fig.height=3, cache=TRUE}
X <- subset(data, select = -y)
y <- data$y
k <- 10
folds <- createFolds(y, k = k)
err<-matrix(0,6,k)
for (i in 1:k) {
  # Sélectionner le pli de données de test
  test_index <- folds[[i]]
  X_test <- X[test_index, ]
  y_test <- y[test_index]
  data.test <- data[test_index, ]
  
  # Sélectionner les données d'entraînement
  train_index <- unlist(folds[-i])
  X_train <- X[train_index, ]
  y_train <- y[train_index]
  data.train <- data[train_index, ]
  
  # Entraîner le modèle SVM sur les données d'entraînement
  model_svm <- svm(x = X_train, y = y_train, kernel = "radial")
  model_linear <- lm(y_train ~ ., data = X_train)
  model_glm <- glm(y_train ~ ., data = X_train, family = gaussian)
  model_spline <- lm(y ~ bs(X1,df=5) + bs(X2,df=5) + bs(X3,df=5) + bs(X4,df=5) +
                    bs(X5,df=5) + bs(X6,df=5) + bs(X7,df=5) + bs(X8,df=5), 
                    data = data[train_index, ])
  model_kernel <- ksvm(y_train ~ ., data = X_train, kernel = "rbfdot", type = "eps-bsvr")
  model_nn <- nnet(y_train ~ ., data = X_train, size = 50,trace=FALSE)
  
  #Prédire les cibles sur les données de test
  predictions_SVM <- predict(model_svm, newdata = X_test)
  predictions_linear <- predict(model_linear, X_test)
  predictions_glm <- predict(model_glm, X_test, type = "response")
  predictions_spline <- predict(model_spline, X_test)
  predictions_kernel <- predict(model_kernel, X_test)
  predictions_nn <- predict(model_nn, newdata=X_test)
  
  for (n in 1:length(test_index)) {
  if (predictions_nn[[n]] > 0.92) {
    predictions_nn[[n]] <- predictions_SVM[[n]]
  }
}

  
  # Calculer l'erreur quadratique moyenne (MSE) sur les données de test
  err[1,i] <- mean((y_test - predictions_SVM) ^ 2)
  err[2,i] <- mean((y_test - predictions_linear) ^ 2)
  err[3,i] <- mean((y_test - predictions_glm) ^ 2)
  err[4,i] <- mean((y_test - predictions_spline) ^ 2)
  err[5,i] <- mean((y_test - predictions_kernel) ^ 2)
  err[6,i] <- mean((y_test - predictions_nn) ^ 2)
  
}
ERR <- data.frame(t(err))
names(ERR) <- c("SVM","Linear","glm","spline","kernel","nn")
boxplot(ERR)
```

Nous remarquons que pour les valeurs prédites supérieurs à 0.9 tendent doucement vers 1. De plus il n'y as pas de valeur supérieure à 1. Pour contrer ce résultat, nous utilisons la prédiction de la méthode SVM (la méthode avec le MSE le plus faible). Un autre avantage est que le MSE du NN sera au maximum égal à celui de la méthode SVM même lorsque la méthode ne converge pas. Nous avons donc décider de garder 3 méthodes pour les tests :le SVM, le KSVM et un réseau de neurones associé au SVM pour les valeurs prédites supérieur à 0.92.

```{r, warning=FALSE, message=FALSE, results='hide',fig.align='center', fig.show='hold', fig.width = 2, fig.height = 4, cache=TRUE}
train_index <- sample(1:nrow(data), 0.8 * nrow(data))
X_train <- X[train_index, ]
y_train <- y[train_index]
X_test <- X[-train_index, ]
y_test <- y[-train_index]

model_svm <- svm(y_train ~ ., data = X_train, kernel = "radial")
model_kernel <- ksvm(y_train ~ ., data = X_train, kernel = "rbfdot", type = "eps-bsvr")
model_nn <- nnet(y_train ~ ., data = X_train, size = 50, trace = FALSE)

predictions_svm <- predict(model_svm, X_test)
predictions_kernel <- predict(model_kernel, X_test)
predictions_nn <- predict(model_nn, X_test)

for (i in 1:(nrow(data)-length(train_index))) {
  if (predictions_nn[[i]] > 0.92) {
    predictions_nn[[i]] <- predictions_svm[[i]]
  }
}

ggplot(data = data.frame(y_test, predictions_svm), aes(x = y_test, y = predictions_svm)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed")
ggplot(data = data.frame(y_test, predictions_kernel), aes(x = y_test, y = predictions_kernel)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed")
ggplot(data = data.frame(y_test, predictions_nn), aes(x = y_test, y = predictions_nn)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed")
```

## Dataset Communities

```{r, cache=TRUE}
library(ggplot2)
library(e1071)
library(splines)
library(kernlab)
library(nnet)
library(dplyr)
library(caret)
library(imputeTS)
data<-read.csv("communities_train.csv")
seed<-87
set.seed(seed)
```

On observe 127 prédicteurs et 1000 observations, certaines doivent être écartées de par leur nature (par ex. communityname). On remarque que des prédicteurs concernent les origines raciales, cela ouvre un débat intéressant quant à l'utlisation de telles données. On remarque également qu'il y a beaucoup de données manquantes, nous allons supprimer les prédicteurs avec trop de valeurs Na, et nous allons imputer (grâce à une régression linéaire) les valeurs manquantes lorsqu'elles concernent moins d'individus.

```{r, warning=FALSE, message=FALSE, fig.align = "center", fig.height=3}
reg.data<-read.csv("communities_train.csv")
reg.data <- subset(reg.data, select = -c(county,community, fold, communityname,state))
set.seed(seed)
pct_na <- colSums(is.na(reg.data)) / nrow(reg.data)
reg.data <- reg.data[, pct_na < 0.8]
na_columns <- colSums(is.na(reg.data)) > 0
for (i in which(na_columns)) {
  model <- lm(reg.data[, i][!is.na(reg.data[, i])] ~ which(!is.na(reg.data[, i])))
  remp <- predict(model, data.frame(x = which(is.na(reg.data[, i]))))
  remp <- ifelse(remp < 0, 0, remp)
  remp <- ifelse(remp > 1, 1, remp)
  reg.data[, i][is.na(reg.data[, i])] <- remp}
indxTrain <- createDataPartition(y = reg.data$ViolentCrimesPerPop, p = 0.8, list = FALSE)
reg.data.train <- reg.data[indxTrain,]
reg.data.test <- reg.data[-indxTrain,]
```

```{r, warning=FALSE, message=FALSE, fig.align = "center", fig.height=3}
set.seed(seed)
ctrl <- trainControl(method = "cv", number = 10)
model.reg.knn <- train(ViolentCrimesPerPop ~ .,  data = reg.data.train,
  method = 'knn',preProcess = c("center", "scale"),
  trControl = ctrl,tuneLength = 20)
pred.knn <- predict(model.reg.knn, newdata = reg.data.test)
test.err.knn <- mean((reg.data.test$ViolentCrimesPerPop - pred.knn)^2)
set.seed(seed)
model.reg.lm <- lm(reg.data.train$ViolentCrimesPerPop ~ ., data = reg.data.train)
pred.lm <- predict(model.reg.lm, newdata = reg.data.test)
pred.lm <- ifelse(pred.lm < 0, 0, pred.lm)
pred.lm <- ifelse(pred.lm >1, 1, pred.lm)
test.err.lm <- mean((reg.data.test$ViolentCrimesPerPop - pred.lm)^2)
set.seed(seed)
```

```{r, warning=FALSE, message=FALSE, fig.align = "center", fig.height=3}
coefs <- summary(model.reg.lm)$coefficients[, 4]
coefs[order(coefs)][1:20]
```

On peut voir qu'une vingtaine de prédicteurs influent particulièrement significativement sur le nombre de crimes, et particulièrement le pourcentage de personnes afro-américain ou encore le salaire des personnes d'origines caucassienne. Cependant, attention aux conclusions hâtives ; elles peuvent être le reflet d'autres prédicteurs cachés.

```{r, warning=FALSE, message=FALSE, fig.align = "center", fig.height=3}
#Sélection de variables
var.signif <- summary(model.reg.lm)$coefficients[, 4] < 0.05
var.signif <- var.signif[-1]
reg.data.a <- reg.data[, c(var.signif, TRUE)]
reg.data.train.a <- reg.data.a[indxTrain,]
reg.data.test.a <- reg.data.a[-indxTrain,]
set.seed(seed)
#Régression linéaire réduite
model.reg.lm.r <- lm(reg.data.train$ViolentCrimesPerPop ~ ., data = reg.data.train.a)
pred.lm.r <- predict(model.reg.lm.r, newdata = reg.data.test.a)
pred.lm.r <- ifelse(pred.lm.r < 0, 0, pred.lm.r)
pred.lm.r <- ifelse(pred.lm.r >1, 1, pred.lm.r)
test.err.lm.r <- mean((reg.data.test$ViolentCrimesPerPop - pred.lm.r)^2)
set.seed(seed)
#LeapForward
model.reg.step.for <- train(ViolentCrimesPerPop ~., data = reg.data.train,method = "leapForward",
  tuneGrid = data.frame(nvmax = 1 : sum(var.signif)),trControl = ctrl)
pred.leap <- predict(model.reg.lm, newdata = reg.data.test)
pred.leap <- ifelse(pred.leap < 0, 0, pred.leap)
pred.leap <- ifelse(pred.leap >1, 1, pred.leap)
test.err.lm <- mean((reg.data.test$ViolentCrimesPerPop - pred.lm)^2)
set.seed(seed)
#Ridge
lambda <- 10^seq(-3, 3, length = 100)
model.reg.ridge <- train(ViolentCrimesPerPop ~ .,data = reg.data.train,
  method = 'glmnet',preProcess = c("center", "scale"),
  trControl = ctrl,tuneGrid = expand.grid(alpha = 0, lambda = lambda))
pred.ridge <- predict(model.reg.ridge, newdata = reg.data.test)
pred.ridge <- ifelse(pred.ridge < 0, 0, pred.ridge)
pred.ridge <- ifelse(pred.ridge >1, 1, pred.ridge)
test.err.ridge <- mean((reg.data.test$ViolentCrimesPerPop - pred.ridge)^2)
set.seed(seed)
#Lasso
lambda <- 10^seq(-3, 3, length = 1000)
model.reg.lasso <- train(ViolentCrimesPerPop ~ .,data = reg.data.train,
  method = 'glmnet',preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda))
pred.lasso <- predict(model.reg.ridge, newdata = reg.data.test)
pred.lasso <- ifelse(pred.lasso < 0, 0, pred.lasso)
pred.lasso <- ifelse(pred.lasso >1, 1, pred.lasso)
test.err.lasso <- mean((reg.data.test$ViolentCrimesPerPop - pred.lasso)^2)
set.seed(seed)
#Elastic
model.reg.elastic <- train(ViolentCrimesPerPop ~ ., data = reg.data.train,method = 'glmnet',preProcess = c("center", "scale"),trControl = ctrl)
pred.elastic <- predict(model.reg.elastic, newdata = reg.data.test)
pred.elastic <- ifelse(pred.elastic < 0, 0, pred.elastic)
pred.elastic <- ifelse(pred.elastic >1, 1, pred.elastic)
test.err.elastic <- mean((reg.data.test$ViolentCrimesPerPop - pred.elastic)^2)
#Spline
cols <- names(reg.data.train.a)[names(reg.data.train.a) != "ViolentCrimesPerPop"]
equation <- "ViolentCrimesPerPop ~"
#Créer une formule avec les prédicteurs qui nous intéressent
for(i in 1:length(cols)) {
  col <- cols[i]
  if(i == length(cols)) {
    equation <- paste(equation, paste0("bs(", col, ", df=1)"))
  } else {
    equation <- paste(equation, paste0("bs(", col, ", df=1) + "))
  }
}
model_spline <- lm(as.formula(equation), data = reg.data.train)
pred.spline <- predict(model_spline, newdata = reg.data.test)
pred.spline <- ifelse(pred.spline < 0, 0, pred.spline)
pred.spline <- ifelse(pred.spline > 1, 1, pred.spline)
test.err.spline <- mean((reg.data.test$ViolentCrimesPerPop - pred.spline)^2)
#Modèle linéaire généralisé
model_glm <- glm(ViolentCrimesPerPop ~ ., data = reg.data.train, family = gaussian)
prediction_glm <- predict(model_glm, reg.data.test, type = "response")
prediction_glm <- ifelse(prediction_glm < 0, 0, prediction_glm)
prediction_glm <- ifelse(prediction_glm > 1, 1, prediction_glm)
test.err.glm <- mean((reg.data.test$ViolentCrimesPerPop - prediction_glm)^2)
set.seed(seed)
#Réseau de neurones
nnet_fit1 <- avNNet(ViolentCrimesPerPop~., data = reg.data.train, size = 10,decay = 0.01, linout = TRUE, trace = F, maxit = 1000,MaxNWts = 5*(1000+1)+ 5+ 1)
pred.nnet.avNNet <- predict(nnet_fit1, reg.data.test)
pred.nnet.avNNet <- ifelse(pred.nnet.avNNet < 0, 0, pred.nnet.avNNet)
pred.nnet.avNNet <- ifelse(pred.nnet.avNNet > 1, 1, pred.nnet.avNNet)
test.err.avnet<- mean((reg.data.test$ViolentCrimesPerPop - pred.nnet.avNNet)^2)
set.seed(seed)
#Réseau de neurones
nnet_fit <- nnet(ViolentCrimesPerPop~., data = reg.data.train, size = 7,decay = 0.01, linout = TRUE, trace = F, maxit = 1200,MaxNWts = 10*(ncol(reg.data.train)+1)+ 5+ 1)
pred.nnet <- predict(nnet_fit, reg.data.test)
pred.nnet <- ifelse(pred.nnet < 0, 0, pred.nnet)
pred.nnet <- ifelse(pred.nnet > 1, 1, pred.nnet)
test.err.nnet <- mean((reg.data.test$ViolentCrimesPerPop - pred.nnet)^2)
set.seed(seed)
train_control <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
#SVM-1
svm_fit <- train(ViolentCrimesPerPop~., data = reg.data.train, method = "svmLinear",preProc = c("center", "scale"), tuneLength = 10,trControl = train_control)
pred.svm1 <- predict(svm_fit, reg.data.test)
pred.svm1 <- ifelse(pred.svm1 < 0, 0, pred.svm1)
pred.svm1 <- ifelse(pred.svm1 > 1, 1, pred.svm1)
test.err.svm1 <- mean((reg.data.test$ViolentCrimesPerPop - pred.svm1)^2)
set.seed(seed)
#SVM-2
grid2 <- expand.grid(C = seq(1,10,length=1))
SVM_fit1 <- train(ViolentCrimesPerPop~.,data=reg.data.train,method="svmLinear",trControl=train_control,tuneGrid = grid2)
pred.svm2 <- predict(SVM_fit1, reg.data.test)
pred.svm2 <- ifelse(pred.svm2 < 0, 0, pred.svm2)
pred.svm2 <- ifelse(pred.svm2 > 1, 1, pred.svm2)
test.err.svm2 <- mean((reg.data.test$ViolentCrimesPerPop - pred.svm2)^2)
set.seed(seed)
#Elastic Net amélioré
en.grid <- expand.grid(lambda=seq(0.001,0.05,length=5),fraction=seq(0.1,0.9,length=5))
model.en1 <-  train(ViolentCrimesPerPop~.,data=reg.data.train,method="enet",trControl=train_control,tuneGrid=en.grid)
pred.en1 <- predict(model.en1, reg.data.test)
pred.en1 <- ifelse(pred.en1 < 0, 0, pred.en1)
pred.en1 <- ifelse(pred.en1 > 1, 1, pred.en1)
test.err.final0 <- mean((reg.data.test$ViolentCrimesPerPop - pred.en1)^2)
#Elastic Net amélioré
reg.data<-read.csv("communities_train.csv")
reg.data <- subset(reg.data, select = -c(county,community, fold, communityname,state))
set.seed(seed)
reg.data <- reg.data[, pct_na < 0.8]
na_columns <- colSums(is.na(reg.data)) > 0
for (i in which(na_columns)) {
  model <- lm(reg.data[, i][!is.na(reg.data[, i])] ~ which(!is.na(reg.data[, i])))
  remp <- predict(model, data.frame(x = which(is.na(reg.data[, i]))))
  remp <- ifelse(remp < 0, 0, remp)
  remp <- ifelse(remp > 1, 1, remp)
  reg.data[, i][is.na(reg.data[, i])] <- remp}
indxTrain <- createDataPartition(y = reg.data$ViolentCrimesPerPop, p = 0.8, list = FALSE)
reg.data.train <- reg.data[indxTrain,]
reg.data.test <- reg.data[-indxTrain,]
en.grid <- expand.grid(lambda=seq(0.001,0.05,length=10),fraction=seq(0.6,1,length=10))
model.communities <-  train(ViolentCrimesPerPop~.,data=reg.data.train,method="enet",trControl=train_control,tuneGrid=en.grid)
dataset <- reg.data.test
  pct_na <- colSums(is.na(dataset)) / nrow(dataset)
  dataset <- dataset[, pct_na < 0.8]
  na_columns <- colSums(is.na(dataset)) > 0
  for (i in which(na_columns)) {
    model <- lm(dataset[, i][!is.na(dataset[, i])] ~ which(!is.na(dataset[, i])))
    remp <- predict(model, data.frame(x = which(is.na(dataset[, i]))))
    remp <- ifelse(remp < 0, 0, remp)
  remp <- ifelse(remp > 1, 1, remp)
    dataset[, i][is.na(dataset[, i])] <- remp}
  predictions <- predict(model.communities, dataset)
  predictions <- ifelse(predictions < 0, 0, predictions)
  #predictions<- round(predictions, 2)
  predictions <- as.data.frame(predictions)
  predictions <- unlist(predictions)
  test.err.final <- mean((reg.data.test$ViolentCrimesPerPop - predictions)^2)
  #Résultat
  cbind(test.err.knn, test.err.lm, test.err.lm.r, test.err.ridge, test.err.lasso, test.err.elastic,test.err.spline,test.err.glm,test.err.avnet,test.err.nnet,test.err.svm1,test.err.svm2,test.err.final0,test.err.final)
```

```{r, warning=FALSE, message=FALSE, fig.align = "center", fig.height=3}
ERR <- data.frame(knn = test.err.knn,lm = test.err.lm,lm.r = test.err.lm.r,
  ridge = test.err.ridge,lasso = test.err.lasso,elastic = test.err.elastic,spline = test.err.spline,glm = test.err.glm,avnet = test.err.avnet,nnet = test.err.nnet,svm1 = test.err.svm1,svm2 = test.err.svm2,elastic.plus = test.err.final0,elastic.plus = test.err.final)
boxplot(ERR)
```

On remarque que l'elastic net est le plus performant. On rajoute une régression linéaire qui estime les valeurs manquantes afin de ne pas perdre de prédicteurs intéressants.

```{r}

```
